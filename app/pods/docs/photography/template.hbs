<div class="relative px-4 sm:px-6 lg:px-8">
    <Heading @title='Photography' @subtitle='Where it all begins'>
        Learning the art and science of photography may go a long way into building an intuition for 3D graphics programming.
        Understanding topics like perspective, lighting, and lenses are crucial to the understanding of 3D rendering.
    </Heading>
    <ProseMarkdown>
        Rendering is an art and before cameras, artists would use techniques to capture images from life, just by observing with their eyes.
        Using grayscale only values (commonly referred to as just "value"), it's possible to project what we see onto a surface and make it appear to have depth.

        Shapes can be constructed not just with lines depicting the edges, but shaded to give the illusion of form. Sharp edges tend to
        have drastic changes in value while soft edges transition more slowly.
    </ProseMarkdown>
    <ProseFigure
        @figureId='1'
        @figureCaption='An illustration of a ball. The soft shading is critical to show that this is a sphere.'
        @imageUrl={{root-url 'figures/artist_illustration_ball.png'}}
    />
    <ProseMarkdown>
        Color alone isn't enough right? No, proportions also matter. The ball could be shaped more rigidly like a box and the edges smoothed
        out but you'll have a tough time convincing me that's a sphere.
    </ProseMarkdown>
    <ProseFigure
        @figureId='2'
        @figureCaption="An illustration of a box but shaded like a sphere. Even with soft shadows, it's not very convincing."
        @imageUrl={{root-url 'figures/artist_illustration_sphere_shaded_box.png'}}
    />
    <ProseMarkdown>
        We perceive proportions in the real world though a lens, our eyes. The shape of this lens and the field of view changes the apparent
        relative sizes of the objects we see. We know they're actually not changing size, that would be silly. Objects further away simply
        get smaller and eventually vanish into the horizon. They vanish because our field of view covers more area the further out it is.
    </ProseMarkdown>
    <ProseFigure
        @figureId='3'
        @figureCaption="An artist's illustration of a 3D scene using perspective guidelines"
        @imageUrl={{root-url 'figures/artist_illustration_perspective.png'}}
    />
    <ProseMarkdown>
        ## Pinhole Camera

        In the early days, a camera was simply a box with a tiny hole to let light through. The diameter of the hole had to be small to help focus
        the light without a lens. An inverted image is produced at the back of the camera box, opposite the hole. Adjusting the image plane distance
        will adjust the **focal length** and **viewing angle**.

        **Focal length** and **field of view** are directly related. Changing one will change the other. Play with this example to get a feel
        for how adjusting these parameters effect the overall image.
    </ProseMarkdown>
    <ProseFigure
        @figureId='4'
        @figureCaption='Interactive visualization of adjusting focal length'
        @componentName='figures/pinhole-camera'
    />
    <ProseMarkdown>
        Hopefully you've played enough with the figure above to get the sense that adjusting the image plane forwards closer to the aperture, the field
        of view gets larger, thereby covering more area. It's zooming out. When moving the image plane backwards, you get the opposite effect.
        It's zooming in.

        ## An Imperfect Camera

        You might be wondering if changing the diameter of the pinhole has any effect. It does, however, in a not so obvious way. The smaller the hole,
        the better the quality of the image, or rather, the sharper the image. The bigger the hole, the bigger the grouping of rays captured and the
        blurrier the image. This grouping of light rays is commonly referred to as the circle of confusion or bokeh. While the diameter effects maximum focal
        length, pinhole sizes are often kept constant and an optimal size is chosen based on the optimum distance to the image plane to keep things in focus.

        Ideally, the perfect diameter would be small enough to limit all but a single ray from each direction. The resulting image would be really
        sharp and each point on the image would correspond to exactly one ray instead of a grouping of rays arriving at random angles. However, this
        would be physically impossible. Such a small pinhole would result in diffraction artifacts as the size approaches the wavelength of the light.

        ## A Better Camera

        Pinhole cameras are nice but they bring with them a slew of issues. Modern day cameras are equipped with a lens to help focus the light unto a
        single point. Unlike a pinhole camera, lenses introduce other artifacts like **depth of field**. **Depth of field** is a property of
        a len's ability to focus outside the focal length or lack of focus to be more accurate. Within the focal range, objects appear sharp but blurry outside of it. This is mostly not an
        issue as people perceive this artifact as astethically pleasing and often buy better lenses to achieve more depth of field easier.
    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/gltf-loader'
        @props={{hash
            modelSrc=(root-url 'models/camera/cam.gltf')
        }}
        @figureId='5'
        @figureCaption='A DSLR camera is a beast of modern technology. They feature variable zoom, variable aperture, auto focus, stabilization, and much more.'
    />
    <ProseMarkdown>

        ## Virtual Cameras

        If photorealism is the goal for a work of art, then you can do no better than with a photocamera. However, this is not easy to achieve when the subject
        being photographed is a work of fiction. Two of the biggest driving force behind recent advancement in computer generated imaging is films and obviously
        games. The two being categorized as offline vs realtime rendering, the latter of which we'll focus on more.

        Generating an image in a computer requires a camera, just like their real-world counterparts. Virtual cameras used in realtime renders, are modeled
        after an ideal pinhole camera. No diffraction effects, and no blur. Computer generated images are ultra sharp as there is infinite depth of field since
        there is no physical lens.

        The viewing volume of this virtual camera is modeled as a **frustum**, as opposed to a cone from the pinhole camera. The cone is not necessary considering
        the light gets captured onto a rectangular screen.

    </ProseMarkdown>
    <ProseFigure
        @figureId='6'
        @figureCaption="A frustum is a pyramid with it's peak cut off. The imaging plane is before the crossing point (nodal point). The image produced is the same whether behind or in front of this nodal point, except that it will not be inverted."
        @componentName='figures/camera-projection'
        @props={{hash enableCameraControls=true}}
    />
    <ProseMarkdown>
        ## Projection

        A virtual camera needs to be able to take in a virtual world and render it out onto a 2-dimensional surface. Of course this also means that we need to be
        able to simulate the subjects in this virtual world. I'll discuss object modeling in a separate chapter but assume for now that every object in the real
        world can be modeled with an array of points in 3D space, a point cloud if you will.

        The mathematics of pinhole projection is relatively simple. Visualize for every point in the scene, a line connecting that point to the
        nodal point. The projected point is exactly where the line intersects the image plane. Geometrically this can be solved with similar triangles.
        Instead of imagining it, just play with it down below.
    </ProseMarkdown>
    <ProseFigure
        @figureId='7'
        @figureCaption="Play with the object's position and notice where it gets projected on the image plane"
        @componentName='figures/camera-projection'
        @props={{hash enableObjectControls=true}}
    />
    <ProseMarkdown>
        ## What About Color?

        We're one stop closer to being able to render a photorealistic scene. We know how points get projected but we must acertain what values each point gets.
        We'll discuss that in the next chapter.
    </ProseMarkdown>
</div>