<div class="relative px-4 sm:px-6 lg:px-8">
    <Heading @title='Object Modeling' @subtitle='The building blocks'>
        The subject of object modeling is more art then science. While I won't be able to cover how best to model
        a dinosaur in this series, I'll focus my attention on the workflow. A mesh can be as simple as a single
        triangle face, or be as complex as a T-rex, rigged, skinned, and textured. In this article, I'll talk
        about what it takes to bring a T-rex to life.
    </Heading>
    <ProseMarkdown>
        We've covered quite a bit already, from camera projections, to lighting and the beginnings of 3D math. Earlier,
        I said you can assume that objects are represented as a point cloud. That's not entirely accurate. Objects
        modeled as point clouds do exist and are commonly used in voxel renderers, but the majority of renderers work on
        polygonal meshes. This article is meant to be fun and give you a look into what it really takes to represent
        a virtual 3D object.

        Let's bring in a T-rex with nothing but vertex information. The most we can do is display the wireframe, or fill it
        with a single color. We don't have normals so it can't be lit with a light source. It doesn't carry with it
        any color information so we can't display it with multiple colors.
    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/object-modeling'
        @figureId='1'
        @figureCaption='Basic wireframe mesh'
        @props={{hash mode='wireframe'}}
    />
    <ProseMarkdown>
        ## Indexed Vertex Data

        Artist would use programs like Maya or Blender to create this data. That data is essentially a list of faces with
        each face containing 3 vertices. A renderer can simply iterate through the list of faces and invoke basic APIs for
        drawing a triangle primitive.

        It's common to not duplicate vertex data considering that faces could share vertices. For example, a box has 12 faces
        (6 sides times 2 triangles per side). Every face contains 3 vertices, a total of 24 vertices. That's a lot of vertices
        when a box could be represented efficiently with 8 vertices.

        Instead of storing 3 vertices per face, it would be better to keep vertex data separate from faces. Faces
        can reference each vertex using it's position in the vertex data buffer, it's index. Most graphics APIs will
        have a way to draw primitives by specifying the indices to the vertices.
    </ProseMarkdown>
    <ProseFigure
        @figureId='2'
        @figureCaption='Vertex data is stored separate from the primitive data (tries). Index data defines the vertices to be used when rendering triangles.'
        @imageUrl={{root-url 'figures/vertex-index-buffer.png'}}
        @vertical={{true}}
    />
    <ProseMarkdown>
        It's not always best to use the same vertex for multiple faces. Normals are commonly grouped together with vertices so
        in the box example with 8 vertices, that can only have 8 normals. There are 12 faces so which normals go to which face?
        It might not be obvious why but all 3 normals associated with a face need to point in the same direction. Otherwise, you
        would get weird shading that might make the triangle look less like a triangle and more like a curved surface. Vertices
        sometimes need to be duplicated to keep the normals separate. Vertices are normally shared when you want to create a smooth
        looking surface. You might see or hear this referred to smoothing groups.

        ## Wireframes Are Boring

        Let's add some life to this model and introduce some color. I've taken the libery to supplement the vertex
        data with color information. That means every vertex would get a color value.
    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/object-modeling'
        @figureId='3'
        @figureCaption='Smooth shading with vertex colors'
        @props={{hash mode='vertex-colors'}}
    />
    <ProseMarkdown>
        ## Lights Make a Big Difference

        Now let's add normals. As I said before, normals is essential for lighting calculations.
    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/object-modeling'
        @figureId='4'
        @figureCaption='Use per-vertex normals so we can give this dinosaur some depth'
        @props={{hash mode='normals'}}
    />
    <ProseMarkdown>
        It's looking much better but something seems off right? If you noticed it doesn't have any self-casted shadows, then
        you're right. It's not practical to using ray-tracing algorithms in real-time just yet. For realtime
        renderers, shadows are added afterwards.

        Outside in real life, even though there is a single directional light source, there are infinitely many
        light sources coming from all directions. Achieving this effect in realtime renderers is not easily done.
        You can add an ambient light source that is suppose to mimic a global light intensity but it's not the same
        and it doesn't produce the soft shadows you see in real life.

        There's still more we can do to this dinosaur to make it look more appealing. Before, we just assigned color
        values to each vertex but we can actually assign a color to each individual pixel.

        ## Textures

        Textures provide a low cost way to provide per-pixel information to a mesh. Without textures, you'd have to
        have a vertex per pixel which could result in eating up your polygon budget. In order to apply a texture to a mesh,
        artists need to tell the renderer how this texture will wrap around the mesh and the process is called UV mapping.

        UV mapping is a process which takes a **3**-dimensional object and unfold it so all it's faces can be spread out
        in **2**-dimensional space. UV stands for the coordinates axes in texture space. Since most textures are **2**-dimensional,
        it has **2** axes <KatexSpan>\boldsymbol{u}</KatexSpan> and <KatexSpan>\boldsymbol{v}</KatexSpan>. Think of UV mapping as
        tearing up your clothes at the seams and laying them out on a flat table.

    </ProseMarkdown>
    <ProseFigure
        @figureId='5'
        @figureCaption='UV mapping is a process of unfolding 3D geometry into a flat plane'
        @imageUrl={{root-url 'figures/uv_mapping.png'}}
        @vertical={{true}}
    />
    <ProseMarkdown>
        Once the UVs are mapped, a texture can be applied.
    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/gltf-loader'
        @props={{hash
            modelSrc=(root-url 'models/texturedtrex/trex.glb')
            modelPosition=(array 0 -5 0)
            modelRotation=(array 0 -90 0)
            hdr=(hash map=(root-url 'textures/equirectangular/crystal_falls_2k.hdr') background=false)
        }}
        @figureId='6'
        @figureCaption='Textures add quite a bit of life into this dinosaur'
    />
    <ProseMarkdown>
        ## Textures Can Improve Lighting

        Textures can be used to improve the horrible lighting quality due to not being able to render with global
        illumination (a technique for rendering soft shadows).

        Not only can we bake per-pixel color info. We can bake **normals** and **AO**. Texture baking is a process transferring
        high resolution information and storing it in an image where it is applied on a lower resolution object. Texture
        baking saves the renderer from having to precompute things on the fly but it also used to add additional input
        from the artist that can't be calculated from vertex data alone.

        Having normals backed allows us to provide curvature data at much higher resolution then a low-poly model can provide.
        Normals can help add cracks and crevices without needing to model in that info. Adding normals don't add any extra
        geometry but is merely an illusion. This is not unlike a painting shaded to give the illusion of depth. A normal map
        is a **24-bit** texture carrying unit directional normals but encoded as **RGB** color

        **AO**, which stands for *ambient occlusion*, is another trick up our sleves. Just as normals can give you a sense of
        greater depth, adding **AO** can add to the effect by shading in areas that are occluded darker. This gives us
        the illusion of shelf-shadows. An *AO* map is gray scale image where black means that area is occluded.

        All these texture maps, except for the color, aren't normally generated by hand. Instead, special programs
        like **zBrush** and **Substance Designer/Painter** are used to either procedurally generate them or allows the artist
        to sculpt directly on the model.

        There are other commonly used maps like metalness and roughness which is used in physically-based shaders to achieve
        realistic renders in realtime.

        Let's see how this dinosaur looks like with these additional textures.

    </ProseMarkdown>
    <ProseFigure
        @componentName='figures/gltf-loader'
        @props={{hash
            modelSrc=(root-url 'models/trex/trex.glb')
            modelPosition=(array 0 -5 0)
            modelRotation=(array 0 -90 0)
            hdr=(hash map=(root-url 'textures/equirectangular/crystal_falls_2k.hdr') background=true)
        }}
        @figureId='7'
        @figureCaption='Rendering using PBR Metalness-Roughness Workflow'
    />
</div>